## Hi there, I'm Siva Kandivalasa ðŸ‘‹ 

ðŸš€ About me

I am highly proficient, versatile and resolution-focused Data Architect/Data Engineer/Cloud Architect offering expertise in data warehousing architecture, lake house, data engineering, data modeling, and project management seeking a challenging role to contribute my expertise and skills to develop scalable and reliable solutions.

ðŸ“Œ Technical Skills

  âœ”ï¸ Snowlfake       ->  Snowsight, SnowSQL, Snowpipe, Streamlit, Snowpark  
  âœ”ï¸ AWS             ->  S3, EC2, Route 53, IAM, Cloud Front, VPC, Auto Scaling, Load Balancing, Lambda, EMR, SQS, SNS, Step Functions, EventBridge, RDS  
  âœ”ï¸ Scripting       ->  SQL, Python, Unix  
  âœ”ï¸ Databases       ->  Snowflake, Teradata, Netezza, SAP HANA, Oracle, DB2, MS-SQL Server(TSQL), MongoDB  
  âœ”ï¸ ETL Tools       ->  Informatica PC, IDMC, Dbt, Matllion, SSIS  
  âœ”ï¸ BI Tools        ->  Tableau, Microstrategy  
  âœ”ï¸ BigData         ->  Databricks, HDFS, MapReduce, Spark, Hive  
  âœ”ï¸ CI/CD Tools     ->  GitHub, BitBucket  
  âœ”ï¸ Schedulers      ->  Tivoli, Tidal, Autosys  

ðŸ† Education & Certifications

  âœ”ï¸ B.Tech (Electronics&Comm) , J.N.T.U, Hyderabad, India  
  âœ”ï¸ SnowPro Advanced Architect & Core Certified, Snowflake  
  âœ”ï¸ AWS 3x Certified (Certified Developer, Solution Architect-Associate and Cloud Practitioner)  
  âœ”ï¸ Python Data Structures - University of Michigan  
  âœ”ï¸ Informatica Power Center Certified Developer, Informatica Corporation  
  âœ”ï¸ Oracle Database 11g: SQL, Oracle Corporation  
  âœ”ï¸ Teradata 12 Certified Professional, Teradata Corporation  
  âœ”ï¸ Informatica Cloud 101 and 201  

Certification Record:   https://www.credly.com/users/siva-kumar-kandivalasa/badges  
                        https://www.credential.net/profile/skandivalasa/wallet

ðŸ“Œ Clients

  âœ”ï¸ Novo Nordisk  
  âœ”ï¸ Merck  
  âœ”ï¸ Tapestry  
  âœ”ï¸ Campbell Soups  
  âœ”ï¸ PVH  
  âœ”ï¸ Wellsfargo  
  âœ”ï¸ Bank Of Tokyo MUFG  
  âœ”ï¸ Principal Financial Group  

ðŸ“Œ Employment

  âœ”ï¸ Brillio                        -> Mar'2024 - Till data  
  âœ”ï¸ Confidential (Consulting)      -> Jan'2014 - Feb'2024  
  âœ”ï¸ Cognizant(CTS)                 -> Oct'2009 - Jan'2014  
  âœ”ï¸ Fujitsu                        -> Oct'2006 - Oct'2009  

ðŸ“Œ Projects:

Client:        Novo Nordisk

Role:          Senior Data Architect\ETL Data Engineer\Lead Febâ€™24 to tilldate

**Project Name:**  EDW - HCP Marketing Systems(Campaigns)

**Summary:**
        â€¢ Accountable for developing and implementing new applications to analyze HCP marketing data for Adobe Experience
        Management (AEP), Adobe Campaign Management (ACM), NBA media, Marketing Mix, and various downstream systems.
        The Enterprise Data Warehouse (EDW) functions as a central repository, aggregating data from both third-party
        and direct vendors containing HCP campaign information. The EDW enhances this data to underpin omnichannel
        dashboards and support any downstream systems that utilize the data.
        
        â€¢ Coordinate with stakeholders, line of business (LoB) teams, and both downstream and upstream owners to gather
        requirements.
        â€¢ Responsible for developing project plans, establishing clear timelines, estimating efforts, identifying system dependencies,
        and collaborating with Scrum Masters
        â€¢ As a project lead, accountable for delivering all project outcomes and ensuring client satisfaction
        â€¢ Designed and implemented ETL/ELT pipelines to transfer vendor files from AWS S3 external buckets to Unix systems,
        from Unix to AWS S3 internal buckets, and ingested data into Snowflake tables
        â€¢ Created and implemented data lake architecture on AWS S3 to consolidate structured and semi-structured data
        â€¢ Created Snowflake stages(internal/external), File Formats, Sequences and Pipes to copy and build ETL pipelines
        â€¢ Created snowflake datashares to share data across multiple teams within the organization
        â€¢ Managed access to database objects via RBAC at various levels and ensured minimum grants provided to users
        â€¢ Applied time travel on core tables for data retention and recovery
        â€¢ Ingesting data using snow pipe using AWS SQS for small workloads
        â€¢ Processed and managed a variety of datasets, including IQVIA, LAAD, and Komodo
        â€¢ Implemented quality control checks on data files, master control files, and metadata to ensure high standards of data
        quality
        â€¢ Responsible for leading issue triaging, root cause analysis, solution design and implementation until issue is resolved
        â€¢ Interact with on-site and off-shore technical resources to elicit business requirements, lead technical design, provide
        guidance on best design practices
        â€¢ Responsible for sprint planning, capacity management, and ADO tracking
        â€¢ Led and mentored a team of 6, overseeing their development and optimizing team performance
        Environment: Snowflake, SnowSQL, Snowpark, Informatica PC, IDMC, AWS S3, AWS SQS, AWS SNS, Python Scripting,
        Postgre SQL, Unix, Azure DavOps and Service Now






<!--
**kshivaetl/kshivaetl** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
