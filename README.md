## Hi there, I'm Siva Kandivalasa üëã 

üöÄ About me

I am highly proficient, versatile and resolution-focused Data Architect/Data Engineer/Cloud Architect offering expertise in data warehousing architecture, lake house, data engineering, data modeling, and project management seeking a challenging role to contribute my expertise and skills to develop scalable and reliable solutions.

üìå Technical Skills

  ‚úîÔ∏è Snowlfake       ->  Snowsight, SnowSQL, Snowpipe, Streamlit, Snowpark  
  ‚úîÔ∏è AWS             ->  S3, EC2, Route 53, IAM, Cloud Front, VPC, Auto Scaling, Load Balancing, Lambda, EMR, SQS, SNS, Step Functions, EventBridge, RDS  
  ‚úîÔ∏è Scripting       ->  SQL, Python, Unix  
  ‚úîÔ∏è Databases       ->  Snowflake, Teradata, Netezza, SAP HANA, Oracle, DB2, MS-SQL Server(TSQL), MongoDB  
  ‚úîÔ∏è ETL Tools       ->  Informatica PC, IDMC, Dbt, Matllion, SSIS  
  ‚úîÔ∏è BI Tools        ->  Tableau, Microstrategy  
  ‚úîÔ∏è BigData         ->  Databricks, HDFS, MapReduce, Spark, Hive  
  ‚úîÔ∏è CI/CD Tools     ->  GitHub, BitBucket  
  ‚úîÔ∏è Schedulers      ->  Tivoli, Tidal, Autosys  

üèÜ Education & Certifications

  ‚úîÔ∏è B.Tech (Electronics&Comm) , J.N.T.U, Hyderabad, India  
  ‚úîÔ∏è SnowPro Advanced Architect & Core Certified, Snowflake  
  ‚úîÔ∏è AWS 3x Certified (Certified Developer, Solution Architect-Associate and Cloud Practitioner)  
  ‚úîÔ∏è Python Data Structures - University of Michigan  
  ‚úîÔ∏è Informatica Power Center Certified Developer, Informatica Corporation  
  ‚úîÔ∏è Oracle Database 11g: SQL, Oracle Corporation  
  ‚úîÔ∏è Teradata 12 Certified Professional, Teradata Corporation  
  ‚úîÔ∏è Informatica Cloud 101 and 201  

Certification Record:   https://www.credly.com/users/siva-kumar-kandivalasa/badges  
                        https://www.credential.net/profile/skandivalasa/wallet

üìå Clients

  ‚úîÔ∏è Novo Nordisk  
  ‚úîÔ∏è Merck  
  ‚úîÔ∏è Tapestry  
  ‚úîÔ∏è Campbell Soups  
  ‚úîÔ∏è PVH  
  ‚úîÔ∏è Wellsfargo  
  ‚úîÔ∏è Bank Of Tokyo MUFG  
  ‚úîÔ∏è Principal Financial Group  

üìå Employment

  ‚úîÔ∏è Brillio                        -> Mar'2024 - Till data  
  ‚úîÔ∏è Confidential (Consulting)      -> Jan'2014 - Feb'2024  
  ‚úîÔ∏è Cognizant(CTS)                 -> Oct'2009 - Jan'2014  
  ‚úîÔ∏è Fujitsu                        -> Oct'2006 - Oct'2009  

üìå Projects:

‚úîÔ∏è Client:        Novo Nordisk          
‚úîÔ∏è Project Name:  EDW - HCP Marketing Systems(Campaigns)
‚úîÔ∏è Summary:  
        ‚Ä¢ Accountable for developing and implementing new applications to analyze HCP marketing data for Adobe Experience
        Management (AEP), Adobe Campaign Management (ACM), NBA media, Marketing Mix, and various downstream systems.
        The Enterprise Data Warehouse (EDW) functions as a central repository, aggregating data from both third-party
        and direct vendors containing HCP campaign information. The EDW enhances this data to underpin omnichannel
        dashboards and support any downstream systems that utilize the data.       
        ‚Ä¢ Coordinate with stakeholders, line of business (LoB) teams, and both downstream and upstream owners to gather
        requirements.
        ‚Ä¢ Responsible for developing project plans, establishing clear timelines, estimating efforts, identifying system dependencies,
        and collaborating with Scrum Masters
        ‚Ä¢ As a project lead, accountable for delivering all project outcomes and ensuring client satisfaction
        ‚Ä¢ Designed and implemented ETL/ELT pipelines to transfer vendor files from AWS S3 external buckets to Unix systems,
        from Unix to AWS S3 internal buckets, and ingested data into Snowflake tables
        ‚Ä¢ Created and implemented data lake architecture on AWS S3 to consolidate structured and semi-structured data
        ‚Ä¢ Created Snowflake stages(internal/external), File Formats, Sequences and Pipes to copy and build ETL pipelines
        ‚Ä¢ Created snowflake datashares to share data across multiple teams within the organization
        ‚Ä¢ Managed access to database objects via RBAC at various levels and ensured minimum grants provided to users
        ‚Ä¢ Applied time travel on core tables for data retention and recovery
        ‚Ä¢ Ingesting data using snow pipe using AWS SQS for small workloads
        ‚Ä¢ Processed and managed a variety of datasets, including IQVIA, LAAD, and Komodo
        ‚Ä¢ Implemented quality control checks on data files, master control files, and metadata to ensure high standards of data
        quality
        ‚Ä¢ Responsible for leading issue triaging, root cause analysis, solution design and implementation until issue is resolved
        ‚Ä¢ Interact with on-site and off-shore technical resources to elicit business requirements, lead technical design, provide
        guidance on best design practices
        ‚Ä¢ Responsible for sprint planning, capacity management, and ADO tracking
        ‚Ä¢ Led and mentored a team of 6, overseeing their development and optimizing team performance
        Environment: Snowflake, SnowSQL, Snowpark, Informatica PC, IDMC, AWS S3, AWS SQS, AWS SNS, Python Scripting,
        Postgre SQL, Unix, Azure DavOps and Service Now






<!--
**kshivaetl/kshivaetl** is a ‚ú® _special_ ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- üî≠ I‚Äôm currently working on ...
- üå± I‚Äôm currently learning ...
- üëØ I‚Äôm looking to collaborate on ...
- ü§î I‚Äôm looking for help with ...
- üí¨ Ask me about ...
- üì´ How to reach me: ...
- üòÑ Pronouns: ...
- ‚ö° Fun fact: ...
-->
